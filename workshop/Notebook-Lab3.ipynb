{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2faab1bd-6d8c-4421-a772-5af8145b709c",
   "metadata": {},
   "source": [
    "## RAG (Retrieval Augmented Generation) Pattern\n",
    "This example demonstrates a Retrieval-Augmented Generation (RAG) pattern using pgVectorStore and LangChain to implement an Aircraft Maintenance Assistance application. It leverages external datasets such as maintenance.csv and aircraft_taxonomy.csv, and uses a privately hosted OpenAI-compatible embedding model to generate embeddings for both queries and text. The chat model retrieves the top three most relevant results from the pgVectorStore embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f353303-be61-42f7-99f7-5a4bfac6406a",
   "metadata": {},
   "source": [
    "### Step 0: Set the working directory for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b49a35b1-3898-4a4e-a48f-b059aef1b7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vcap/app/cf-jupyterlab-workshop\n"
     ]
    }
   ],
   "source": [
    "# set the working directory for the project\n",
    "%cd /home/vcap/app/cf-jupyterlab-workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db5e9e-da2a-4bd4-a2b5-bb2781c2710b",
   "metadata": {},
   "source": [
    "### Step 1: Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "590a345b-4a56-4ef7-88dc-5192ef7ff731",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ingestion pipeline to load data\n",
    "## Install following Modules - langchain-postgres, psycopg2\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import httpx\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from cfenv import AppEnv\n",
    "import sys, os\n",
    "import warnings\n",
    "from tanzu_utils import CFGenAIService\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9e694-95cc-4391-9a7c-e393119fe366",
   "metadata": {},
   "source": [
    "### Step 2: Set up the OpenAI compatible embedding model credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c96d5555-37ac-416e-9fff-7889dfc8998a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nomic-embed-text-v1025 (capabilities: EMBEDDING)\n",
      "Embedding model: nomic-embed-text-v1025\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Load services from env\n",
    "# -----------------------------\n",
    "env = AppEnv()\n",
    "# -----------------------------\n",
    "# Embedding service details\n",
    "# -----------------------------\n",
    "embedding_service = CFGenAIService(\"tanzu-nomic-embed-text\")\n",
    "\n",
    "# List available models\n",
    "embedding_models = embedding_service.list_models()\n",
    "for m in embedding_models:\n",
    "    print(f\"- {m['name']} (capabilities: {', '.join(m['capabilities'])})\")\n",
    "\n",
    "\n",
    "api_base = embedding_service.api_base + \"/openai/v1\"\n",
    "api_key = embedding_service.api_key\n",
    "model_name = embedding_models[0][\"name\"]\n",
    "\n",
    "print(\"Embedding model:\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d88495-a857-47bd-a97b-adfbd19f7252",
   "metadata": {},
   "source": [
    "### Step 3: Connect to pgvector store and test the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42da6211-1f91-430b-a8df-30b42c40cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: PostgreSQL 16.6 (VMware Postgres 16.6.0) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, 64-bit\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Database connection\n",
    "# -----------------------------\n",
    "db_service = env.get_service(name=\"vector-db\")\n",
    "db_credentials = db_service.credentials\n",
    "db_uri = db_credentials[\"uri\"]\n",
    "\n",
    "engine = create_engine(db_uri)\n",
    "\n",
    "# Test DB connection\n",
    "with engine.connect() as conn:\n",
    "    version = conn.execute(text(\"SELECT version();\")).fetchone()\n",
    "    print(\"Connected to:\", version[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a444738-3c68-4cf4-affa-a236912062ac",
   "metadata": {},
   "source": [
    "### Step 4: Initialize a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fb556e-1b6f-4c48-8a3a-be8eaab7f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Embedding function (REST call)\n",
    "# -----------------------------\n",
    "url = api_base + \"/embeddings\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "def embed_text(text: str):\n",
    "    payload = {\"model\": model_name, \"input\": text}\n",
    "    resp = requests.post(url, headers=headers, json=payload, verify=False)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def embed_documents(self, texts): return [embed_text(t) for t in texts]\n",
    "    def embed_query(self, text): return embed_text(text)\n",
    "\n",
    "embedding = CustomEmbeddings()\n",
    "\n",
    "# -----------------------------\n",
    "# PGVector setup\n",
    "# -----------------------------\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embedding,\n",
    "    connection=db_uri,\n",
    "    collection_name=\"maintenance_and_taxonomy\",\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a004285-522d-4126-929c-f3401096f653",
   "metadata": {},
   "source": [
    "### Step 5: Set up OpenAI compatible chat model credentials and initialize a chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3b799bf-e784-4dea-9e0b-155396b9d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- openai/gpt-oss-120b (capabilities: CHAT, TOOLS)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import httpx\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datetime import date\n",
    "import warnings\n",
    "import ssl\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Optional: configure custom http client\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=30.0)\n",
    "# -----------------------------\n",
    "# cat service details\n",
    "# -----------------------------\n",
    "chat_service = CFGenAIService(\"tanzu-gpt-oss-120b\")\n",
    "\n",
    "# List available models\n",
    "chat_models = chat_service.list_models()\n",
    "for m in chat_models:\n",
    "    print(f\"- {m['name']} (capabilities: {', '.join(m['capabilities'])})\")\n",
    "\n",
    "chat_api_base = chat_service.api_base + \"/openai/v1\"\n",
    "chat_api_key = chat_service.api_key\n",
    "chat_model_name = chat_models[0][\"name\"]\n",
    "\n",
    "# Initialize LLM with credentials from cfenv\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.9,\n",
    "    model=chat_model_name,\n",
    "    base_url=chat_api_base,\n",
    "    api_key=chat_api_key,\n",
    "    http_client=httpx_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82408b38-c47b-4792-a039-62e905cffdb9",
   "metadata": {},
   "source": [
    "### Step 6: Create a retriever from your vectorstore and execute a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "754e697f-cc05-4a03-a849-4a752d1f2701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reported hydraulic‑leak issue is with **Landing Gear B**.\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from your vectorstore\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "# Build a RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"Which aircraft equipment has reported issues with hydraulic leaks?\"\n",
    "result = qa.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494085d5-b1f3-456c-a8ab-e50908244cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
