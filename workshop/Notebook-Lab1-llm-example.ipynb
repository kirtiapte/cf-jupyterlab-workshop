{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b98406c-0f47-46bd-aa9c-2138e27e909a",
   "metadata": {},
   "source": [
    "## Working with LLM Programatically: LangChain with Simple Chain Pattern\n",
    "This example demonstrates how to configure a LangChain setup that connects to a OpenAI compatible LLM private endpoint using environment based credentials, and then prompts the model to generate a short overview of “VMware Tanzu Hub.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d1dac-16ef-433d-a532-1bf4bb4774e2",
   "metadata": {},
   "source": [
    "### Step 0: Set the working directory for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "310c2101-053f-4a53-951b-c87f694f6519",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vcap/app/cf-jupyterlab-workshop\n"
     ]
    }
   ],
   "source": [
    "# set the working directory for the project\n",
    "%cd /home/vcap/app/cf-jupyterlab-workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27b902-b872-4018-ab5c-1daac03aaae3",
   "metadata": {},
   "source": [
    "### Step 1: Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bc1810-b5d7-4970-8b76-86c0e5e9ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import json\n",
    "import httpx\n",
    "import warnings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tanzu_utils import CFGenAIService\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c641b",
   "metadata": {},
   "source": [
    "### Step 2: Set up the OpenAI API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d71ffb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- openai/gpt-oss-120b (capabilities: CHAT, TOOLS)\n"
     ]
    }
   ],
   "source": [
    "# load your service details replace name with your Gen AI service.  Gen AI service is bound to the app\n",
    "chat_service = CFGenAIService(\"tanzu-gpt-oss-120b\")\n",
    "\n",
    "# List available models\n",
    "models = chat_service.list_models()\n",
    "for m in models:\n",
    "    print(f\"- {m['name']} (capabilities: {', '.join(m['capabilities'])})\")\n",
    "\n",
    "# construct chat_credentials\n",
    "chat_credentials = {\n",
    "    \"api_base\": chat_service.api_base + \"/openai/v1\",\n",
    "    \"api_key\": chat_service.api_key,\n",
    "    \"model_name\": models[0][\"name\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae906b",
   "metadata": {},
   "source": [
    "### Step 3: Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba3033b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. HTTP client (optional but recommended for custom config)\n",
    "httpx_client = httpx.Client(verify=False)  # verify=False if your endpoint needs --insecure\n",
    "\n",
    "# 3. Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.9,\n",
    "    model=chat_credentials[\"model_name\"],   # model name from CF service\n",
    "    base_url=chat_credentials[\"api_base\"],  # OpenAI-compatible endpoint\n",
    "    api_key=chat_credentials[\"api_key\"],    # Bearer token\n",
    "    http_client=httpx_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3832a",
   "metadata": {},
   "source": [
    "### Step 4: Create a prompt template, create a chain, and run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd2b54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Purpose:** VMware Tanzu Hub serves as a unified control plane that streamlines the provisioning, governance, and lifecycle management of modern applications across multiple Kubernetes clusters.  \n",
      "- **Key Capabilities:** Centralized cluster registration, automated policy enforcement, multi‑cluster observability, and secure, role‑based access to workloads and infrastructure.  \n",
      "- **Core Components:** Tanzu Hub API server, the Tanzu Hub UI dashboard, a policy engine (OPA‑based), and an integration layer for VMware Tanzu Mission Control, Tanzu Kubernetes Grid, and Tanzu Observability.  \n",
      "- **Ecosystem Integration:** It connects seamlessly with Tanzu Application Platform, Tanzu Build Service, and Tanzu Service Mesh, providing a single source of truth for cataloged services and shared configuration.  \n",
      "- **Benefit:** By consolidating management of disparate Kubernetes environments, Tanzu Hub accelerates developer velocity while maintaining enterprise‑grade security and compliance across the entire Tanzu portfolio.\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Provide a concise 5-line overview of {topic}.\n",
    "    Focus on its purpose, key capabilities, core components,\n",
    "    and how it integrates within the VMware Tanzu ecosystem.\n",
    "    Write in a clear, professional tone suitable for a technical summary.\"\"\"\n",
    ")\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define the topic\n",
    "topic = \"VMware Tanzu Hub\"\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(topic)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e80f6-7990-4cbd-a020-e9dfd8531600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
